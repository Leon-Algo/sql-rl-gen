def compute_reward(self, input_item, predicted_text) -> Tuple[float, Dict]:
    """
    Compute reward for converting question and database tables into SQL query.

    Args:
        input_item (dict): Input item containing the question.
        predicted_text (str): Predicted SQL query generated by the model.

    Returns:
        Tuple[float, Dict]: Reward value and a dictionary of metrics.
    """

    # Initialize reward and metrics
    reward = 0.0
    metrics = {}

    # Check if the predicted query is empty
    if not predicted_text.strip():
        return -10.0, {"error_type": "empty_query"}

    # Compute accuracy
    accuracy = self.sql_query_execution_feedback(input_item['input'], predicted_text)
    metrics["accuracy"] = accuracy.get("accuracy", 0.0)

    # Check for errors in the query execution
    if accuracy.get("error_type") != "present":
        reward -= abs(accuracy.get("f1", 0.0) - 1.0)
        metrics["error_type"] = accuracy.get("error_type")
        metrics["error_reason"] = accuracy.get("error_reason")

    # Compute precision, recall, and F1 score
    if accuracy.get("precision") != -1:
        reward += abs(accuracy.get("precision", 0.0) - 1.0)
    if accuracy.get("recall") != -1:
        reward += abs(accuracy.get("recall", 0.0) - 1.0)
    if accuracy.get("f1") != -1:
        reward += abs(accuracy.get("f1", 0.0) - 1.0)

    # Compute IoU
    iou = accuracy.get("iou", 0.0)
    reward += abs(iou - 1.0)

    # Give a penalty for missing keywords in the query
    base_penalty = 5.0
    penalty = self.missed_key_words(base_penalty, input_item['input'], predicted_text)
    reward -= penalty

    return reward, metrics

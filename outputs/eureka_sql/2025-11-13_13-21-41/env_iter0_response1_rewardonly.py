def compute_reward(self, input_item, predicted_text) -> Tuple[float, Dict]:
    """
    Compute reward for converting question and database tables into SQL query.

    Args:
        input_item (dict): Input item containing the question.
        predicted_text (str): Predicted SQL query generated by the model.

    Returns:
        Tuple[float, Dict]: Reward value and a dictionary of metrics.
    """

    # Initialize reward and metrics
    reward = 0.0
    metrics = {}

    # Check if the predicted query is empty
    if not predicted_text.strip():
        return -10.0, {"error_type": "empty_query"}

    # Compute accuracy using sql_query_execution_feedback function
    feedback_dict = self.sql_query_execution_feedback(input_item, predicted_text)
    accuracy = feedback_dict.get("accuracy", 0.0)
    metrics["accuracy"] = accuracy

    # Check if the query is correct
    if accuracy == 1.0:
        reward = 10.0
    elif accuracy > 0.8:
        reward = 5.0
    else:
        reward = -5.0

    # Compute precision, recall, and F1 score using sql_query_execution_feedback function
    precision = feedback_dict.get("precision", 0.0)
    recall = feedback_dict.get("recall", 0.0)
    f1 = feedback_dict.get("f1", 0.0)
    metrics["precision"] = precision
    metrics["recall"] = recall
    metrics["f1"] = f1

    # Check if the query is correct
    if precision > 0.8 and recall > 0.8:
        reward += 5.0
    elif precision > 0.6 and recall > 0.6:
        reward += 2.0
    else:
        reward -= 3.0

    # Check for missing keywords using missed_key_words function
    penalty = self.missed_key_words(10.0, input_item["input"], predicted_text)
    metrics["penalty"] = penalty
    if penalty > 0.0:
        reward += penalty

    # Compute IoU and error type using sql_query_execution_feedback function
    iou = feedback_dict.get("iou", 0.0)
    error_type = feedback_dict.get("error_type", "")
    metrics["iou"] = iou
    metrics["error_type"] = error_type

    # Check if the query is correct
    if iou > 0.8:
        reward += 5.0
    elif iou > 0.6:
        reward += 2.0
    else:
        reward -= 3.0

    # Save metrics to CSV file
    save_dict_csv(metrics, self.output_dir, f"{self.environment_name}_metrics.csv")

    return reward, metrics
